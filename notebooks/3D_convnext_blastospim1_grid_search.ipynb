{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57f16ff-f34c-4c01-9ef0-e6996141e5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "__init__.py (48): Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch StarDist3D\n",
    "sys.path.append('..')\n",
    "from pytorch_stardist.data.utils import normalize\n",
    "from pytorch_stardist.models.config import Config3D\n",
    "from pytorch_stardist.models.stardist3d import StarDist3D\n",
    "from utils import seed_all, prepare_conf\n",
    "\n",
    "from stardist_tools.matching import matching_dataset\n",
    "\n",
    "# Need this even when not using multiprocessing\n",
    "os.environ[\"LOCAL_RANK\"] = '0'\n",
    "os.environ[\"RANK\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-load-markdown",
   "metadata": {},
   "source": [
    "### 1. Load Configuration and Pre-trained Model\n",
    "Load the model configuration from the YAML file and instantiate the `StarDist3D` model with pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6510aac6-e91d-44b3-980a-c512853deffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = '../confs/train_convnext_unet_base-3D.yaml'\n",
    "\n",
    "with open(config_file) as yconf:\n",
    "    opt = yaml.safe_load(yconf)\n",
    "\n",
    "Config = Config3D\n",
    "StarDist = StarDist3D\n",
    "\n",
    "conf = Config(**opt, allow_new_params=True)\n",
    "\n",
    "# Set random seed\n",
    "seed_all(conf.random_seed)\n",
    "\n",
    "# process the configuration variables\n",
    "opt = prepare_conf(conf)\n",
    "\n",
    "# Model instanciation\n",
    "model = StarDist(opt)\n",
    "model.net.load_state_dict(torch.load('../model_checkpoints/convnext_unet_base-3D.pth'))\n",
    "model.net.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-def-markdown",
   "metadata": {},
   "source": [
    "### 2. Define and Prepare the Dataset\n",
    "We define the `BlastospimDataset` class to load images and masks. For a robust grid search, all available test sets are combined. We also load the ground truth masks into memory once to speed up the evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c9295a-b088-417b-bd70-a46fc693e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlastospimDataset(Dataset):\n",
    "    def __init__(self, image_names, source_dir):\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        for name in image_names:\n",
    "            self.image_paths.append(f'{source_dir}/{name}/{name}/images/{name}_image_0001.npy')\n",
    "            self.mask_paths.append(f'{source_dir}/{name}/{name}/masks/{name}_masks_0001.npy')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        image = np.load(image_path)\n",
    "        mask = np.load(mask_path)\n",
    "\n",
    "        # Make image dimensions divisible by n:\n",
    "        n = 32\n",
    "        image = image[:image.shape[0]-image.shape[0]%n, :image.shape[1]-image.shape[1]%n, :image.shape[2]-image.shape[2]%n]\n",
    "        mask = mask[:mask.shape[0]-mask.shape[0]%n, :mask.shape[1]-mask.shape[1]%n, :mask.shape[2]-mask.shape[2]%n]\n",
    "        assert image.shape == mask.shape\n",
    "\n",
    "        # Normalize image\n",
    "        axis_norm = (0, 1, 2)  # normalize channels independently\n",
    "        image = np.expand_dims(normalize(image, 1, 99.8, axis=axis_norm), 0) # Add channel for one color\n",
    "\n",
    "        return {\n",
    "            'image':image.astype(np.float32),\n",
    "            'mask':mask.astype(np.int16)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8164d47-4dc1-4491-86c8-6ef6af732f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/mnt/ceph/users/alu10/datasets/GTSets/2023_Full_Iso-Trilinear_Image'\n",
    "\n",
    "testset8 = ['F24_001', 'F24_002', 'F24_006', 'F25_002', 'F25_008', 'F27_010', 'F27_007', 'F27_009', 'F29_003','F29_004', 'F30_004', 'F30_008', 'F30_009', 'M6_021', 'M6_012']\n",
    "testset16 = ['M7_004', 'M7_000', 'F42_063', 'F41_056', 'F34_073', 'F33_067', 'F26_008', 'F24_010']\n",
    "testset32 = ['F8_072', 'F44_087', 'F44_089', 'F39_117']\n",
    "testset64 = ['F40_136', 'F49_148']\n",
    "testset128 = ['F55_185']\n",
    "\n",
    "# Combine all test sets for a comprehensive evaluation\n",
    "all_test_names = testset8 + testset16 + testset32 + testset64 + testset128\n",
    "\n",
    "test_dataset = BlastospimDataset(all_test_names, source_dir)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load all GT masks into memory once\n",
    "gt_masks = [data['mask'][0].numpy() for data in tqdm(test_dataloader, desc=\"Loading GT masks\")]\n",
    "\n",
    "print(f'Combined test set contains {len(test_dataset)} images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grid-search-markdown",
   "metadata": {},
   "source": [
    "### 3. Perform Grid Search\n",
    "Now we loop through different combinations of `prob_thresh` and `nms_thresh`. For each combination, we generate instance labels for the *entire dataset* and calculate the F1-score at IoU=0.7.\n",
    "\n",
    "**⚠️ Warning:** This method is computationally expensive. It re-runs the full prediction pipeline for every image at every point in the grid. This may take a very long time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grid-search-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of thresholds to search\n",
    "prob_thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "nms_thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "iou_thresh_fixed = 0.7\n",
    "\n",
    "results = []\n",
    "grid = list(itertools.product(prob_thresholds, nms_thresholds))\n",
    "\n",
    "print(f\"Starting grid search over {len(grid)} combinations...\")\n",
    "for prob_thresh, nms_thresh in tqdm(grid, desc=\"Grid Search Progress\"):\n",
    "    # Set the model's thresholds for this iteration\n",
    "    model.thresholds['prob'] = prob_thresh\n",
    "    model.thresholds['nms'] = nms_thresh\n",
    "    \n",
    "    predicted_labels = []\n",
    "    # Loop through the dataset and predict for each image\n",
    "    for batch in test_dataloader:\n",
    "        image = batch['image'][0].numpy()\n",
    "        labels, _ = model.predict_instance(image, patch_size=[256, 256, 256], context=[64, 64, 64])\n",
    "        predicted_labels.append(labels)\n",
    "        \n",
    "    # Evaluate this set of predictions against all ground truth masks\n",
    "    stats = matching_dataset(gt_masks, predicted_labels, thresh=iou_thresh_fixed, show_progress=False)\n",
    "    \n",
    "    results.append({\n",
    "        'prob_thresh': prob_thresh,\n",
    "        'nms_thresh': nms_thresh,\n",
    "        'iou_thresh': iou_thresh_fixed,\n",
    "        'f1': stats.f1,\n",
    "        'precision': stats.precision,\n",
    "        'recall': stats.recall\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze and Visualize Results\n",
    "First, we find and print the best combination of thresholds that maximized the F1-score. Then, we create a heatmap to visualize the performance across the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "analysis-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best result\n",
    "best_result = results_df.loc[results_df['f1'].idxmax()]\n",
    "\n",
    "print(\"--- Grid Search Complete ---\")\n",
    "print(f\"Best F1-Score: {best_result['f1']:.4f}\")\n",
    "print(f\"Optimal prob_thresh: {best_result['prob_thresh']:.2f}\")\n",
    "print(f\"Optimal nms_thresh: {best_result['nms_thresh']:.2f}\")\n",
    "print(f\"Precision at best F1: {best_result['precision']:.4f}\")\n",
    "print(f\"Recall at best F1: {best_result['recall']:.4f}\")\n",
    "\n",
    "# Pivot the data for the heatmap\n",
    "f1_pivot = results_df.pivot(index='prob_thresh', columns='nms_thresh', values='f1')\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(f1_pivot, annot=True, fmt=\".3f\", cmap=\"viridis\", cbar_kws={'label': 'F1-Score'})\n",
    "plt.title(f'F1-Score Grid Search (IoU Threshold = {iou_thresh_fixed})')\n",
    "plt.xlabel('NMS Threshold')\n",
    "plt.ylabel('Probability Threshold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0159d5-acf9-4ce2-8861-d22bb30714dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stardist",
   "language": "python",
   "name": "stardist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
